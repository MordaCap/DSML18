{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df['datetime'] =  pd.to_datetime(df['datetime'])\n",
    "df = df.drop(['registered', 'casual'], axis=1)\n",
    "df = df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Немного посмотрели предварительно на все int cols, заметили магией, что можно их в onehot пихнуть (просто подумали, а какие из них категории, а какие не торт)\n",
    "### Стоит сказать, что onehot из sklearn мне нравится меньше dummes из pd, но справедливости ради нужно уметь работать со спарсами, память-***мять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "y = df['count']\n",
    "x = df.drop('count', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51131.67176974893"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "mean_squared_error(y_true=y_test, y_pred=model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37407.3574814588"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "mean_squared_error(y_true=y_test, y_pred=model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonCatCols = ['temp', 'atemp','humidity', 'windspeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n",
      "/home/morda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "enc.fit(df[['season', 'holiday', 'workingday', 'weather']])\n",
    "\n",
    "x_train_oneHot = enc.transform(x_train[['season', 'holiday', 'workingday', 'weather']])\n",
    "x_test_oneHot = enc.transform(x_test[['season', 'holiday', 'workingday', 'weather']])\n",
    "x_train_oneHot = hstack((x_train_oneHot, x_train[nonCatCols].as_matrix()))\n",
    "x_test_oneHot = hstack((x_test_oneHot, x_test[nonCatCols].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45398.20055113288"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLR = LogisticRegression()\n",
    "modelLR.fit(x_train_oneHot, y_train)\n",
    "\n",
    "mean_squared_error(y_true=y_test, y_pred=modelLR.predict(x_test_oneHot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37913.874364241"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(x_train_oneHot, y_train)\n",
    "\n",
    "mean_squared_error(y_true=y_test, y_pred=model.predict(x_test_oneHot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# го на feature_importances_ посмотрим ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.242601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.190560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.145518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.024569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.023887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.017593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.009194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Importance\n",
       "14    0.247700\n",
       "13    0.242601\n",
       "15    0.190560\n",
       "12    0.145518\n",
       "3     0.030243\n",
       "7     0.024569\n",
       "9     0.023887\n",
       "1     0.023268\n",
       "8     0.017593\n",
       "2     0.014649\n",
       "6     0.013036\n",
       "0     0.009629\n",
       "10    0.009194\n",
       "5     0.004253\n",
       "4     0.003300\n",
       "11    0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = model.feature_importances_\n",
    "importance = pd.DataFrame(importance, \n",
    "                          columns=[\"Importance\"])\n",
    "importance.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [11,5,4,10,0,7]\n",
    "good = [14,13,15,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37569.161361765175\n",
      "37531.89231645914\n"
     ]
    }
   ],
   "source": [
    "xtr_new = sparse.lil_matrix(sparse.csr_matrix(x_train_oneHot)[:,[i for i in range(16) if i in good]])\n",
    "xts_new = sparse.lil_matrix(sparse.csr_matrix(x_test_oneHot)[:,[i for i in range(16) if i in good]])\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(xtr_new, y_train)\n",
    "\n",
    "print(mean_squared_error(y_true=y_test, y_pred=model.predict(xts_new)))\n",
    "\n",
    "xtr_new = sparse.lil_matrix(sparse.csr_matrix(x_train_oneHot)[:,[i for i in range(16) if i not in bad]])\n",
    "xts_new = sparse.lil_matrix(sparse.csr_matrix(x_test_oneHot)[:,[i for i in range(16) if i not in bad]])\n",
    "\n",
    "model.fit(xtr_new, y_train)\n",
    "\n",
    "print(mean_squared_error(y_true=y_test, y_pred=model.predict(xts_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и протравим модельке из прошлого таска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22222.38944446594"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeRegressor(max_depth=6,max_leaf_nodes=50)\n",
    "model.fit(xtr_new,y_train)\n",
    "\n",
    "mean_squared_error(y_true=y_test, y_pred=model.predict(xts_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Немного бэклога:\n",
    "    - LogReg просто и с onehot (mse): 51131.67 vs. 45398.20 --- (чисто из коробки очень крутой буст, просто с oneHot)\n",
    "    - DT (Ну тут можно долго играться, бустинги накрутить, данные получше повертеть с разных сторон и вообще включить кеглера, а можно просто посмотреть, как линейные регрессии проигрывают деревьям в анализе почти 80% катфитч (selfjoke Catboost)) :::: DT vs DT+OneHot (mse) : 37407.35 vs. 37913.87. Почему так? Ну какую-никакую, но importance обрезанные фичи имели. Зачем так делать? Еще один способ удалить неважный мусор и сосредоточиться на важном. Ну и т.д. \n",
    "    \n",
    "    onehot encoding отличный инструмент в различных ситуациях, не только в регресии. Позволяет алгоритмам почуствовать себя деревянными. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
